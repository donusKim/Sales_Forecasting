{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sales_Forecasting(With CNN).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donusKim/Sales_Forecasting/blob/master/Sales_Forecasting(With_CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kJHFJ4l9Rwc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2566397-2443-4256-b755-10722be7d92a"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAi8YKSE9b4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Data Preprocessing ####\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import gc\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import datetime\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras import optimizers\n",
        "from datetime import date, timedelta\n",
        "\n",
        "\n",
        "df=pd.read_csv(\"./gdrive/My Drive/롯데백화점/purchase_data.csv\",encoding=\"ms949\" )\n",
        "temp=pd.read_csv(\"./gdrive/My Drive/롯데백화점/기온2.csv\",encoding=\"ms949\").set_index([\"지점\"])\n",
        "rain=pd.read_csv(\"./gdrive/My Drive/롯데백화점/강수량2.csv\",encoding=\"ms949\").set_index([\"지점\"])\n",
        "cpi=pd.read_csv(\"./gdrive/My Drive/롯데백화점/소비자물가지수.csv\",encoding=\"ms949\").set_index([\"시도별\"])\n",
        "temp.drop(\"160\",axis=1,inplace=True)\n",
        "rain.drop(\"160\",axis=1,inplace=True)\n",
        "\n",
        "# Weekly Data \n",
        "import datetime\n",
        "df[\"days\"]=df['date'].apply(lambda x: datetime.datetime.strptime(str(x),'%Y-%m-%d'))\n",
        "df[\"days\"]=df[\"days\"]-np.datetime64('2016-07-03')\n",
        "df[\"week\"]=df[\"days\"]/7\n",
        "\n",
        "\n",
        "## 취소매출 제거\n",
        "df[\"absolute\"]=abs(df[\"pur_amt\"])\n",
        "df=df.drop_duplicates([\"cus_id\",\"absolute\"], keep='first')\n",
        "new_df=df[[\"point\",\"week\",\"brand\",\"absolute\"]]\n",
        "\n",
        "\n",
        "####점 이름 바꿔주기 ####\n",
        "substitutions = {1.0: \"본점\",2.0: \"잠실점\",5.0: \"부산본점\",6.0: \"관악점\",8.0: \"분당점\",10.0: \"영등포점\",11.0: \"일산점\",13.0: \"강남점\",17.0: \"창원점\",22.0: \"노원점\",28.0: \"건대스타점\",333.0: \"광복점\"\n",
        "                 ,341.0: \"평촌점\",344.0: \"인천터미널점\"}        \n",
        "new_df=new_df.replace(substitutions)\n",
        "\n",
        "\n",
        "new_df[\"week\"]=new_df[\"week\"].apply(lambda x: x.days)\n",
        "new_df=new_df[(new_df[\"week\"]!=160) & (new_df[\"week\"]!=-1) ]\n",
        "\n",
        "#영등포점 오입력 data 제거\n",
        "new_df.drop(28391,axis=0,inplace=True)\n",
        "new_df.drop(224708,axis=0,inplace=True)\n",
        "\n",
        "#전처리 위해 reindex\n",
        "new_df=new_df.set_index([\"point\",\"brand\",\"week\"]).groupby(level=[0,1,2])['absolute'].agg({('sum',np.sum)}).unstack(level=-1).fillna(0)\n",
        "new_df.columns=new_df.columns.get_level_values(1)\n",
        "\n",
        "new_df=new_df[sorted(new_df.columns)]\n",
        "\n",
        "#관악점, 인천터미널점 제거\n",
        "new_df.drop(\"관악점\",axis=0,inplace=True)\n",
        "new_df.drop(\"인천터미널점\",axis=0,inplace=True)\n",
        "\n",
        "\n",
        "temp=temp.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
        "rain=rain.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
        "\n",
        "cpi.loc[\"전국\"]=cpi.loc[\"경기도\"]\n",
        "cpi.index=[\"경기북부\",\"서울\",\"부산\",\"인천\",\"경기남부\",\"창원\"]\n",
        "\n",
        "#인천제거\n",
        "rain.drop(\"인천\",axis=0,inplace=True)\n",
        "cpi.drop(\"인천\",axis=0,inplace=True)\n",
        "temp.drop(\"인천\",axis=0,inplace=True)\n",
        "\n",
        "\n",
        "point=pd.read_csv(\"./gdrive/My Drive/롯데백화점/point_table.csv\",encoding=\"ms949\").set_index([\"point_name\"])\n",
        "\n",
        "\n",
        "rain_new=pd.merge(point, rain,left_on='지역', right_index=True,how='left')\n",
        "temp_new=pd.merge(point, temp,left_on='지역', right_index=True,how='left')\n",
        "temp_new.drop(\"지역\",axis=1,inplace=True)\n",
        "rain_new.drop(\"지역\",axis=1,inplace=True)\n",
        "\n",
        "temp_reindex=temp_new.reindex(new_df.index.get_level_values(0))\n",
        "rain_reindex=rain_new.reindex(new_df.index.get_level_values(0))\n",
        "\n",
        "\n",
        "new_df.columns=pd.date_range(start='6/30/2016', periods=160, freq='W')\n",
        "temp_reindex.columns=new_df.columns\n",
        "temp_reindex.columns=new_df.columns\n",
        "\n",
        "\n",
        "# 3년간 기록있는 Brand만 활용\n",
        "temp_reindex.index=new_df.index\n",
        "rain_reindex.index=new_df.index\n",
        "\n",
        "\n",
        "too_many_zero_index=new_df[((new_df['2016-07-03']==0) &(new_df['2016-07-10']==0) &(new_df['2016-07-17']==0)&(new_df['2016-07-24']==0)) | ((new_df['2019-06-30']==0) &(new_df['2019-07-07']==0) &(new_df['2019-07-14']==0)&(new_df['2019-07-21']==0)) ].index\n",
        "\n",
        "\n",
        "\n",
        "temp_reindex=temp_reindex.loc[(new_df.index).difference(too_many_zero_index)]\n",
        "rain_reindex=rain_reindex.loc[(new_df.index).difference(too_many_zero_index)]\n",
        "df=new_df\n",
        "new_df=new_df.loc[(new_df.index).difference(too_many_zero_index)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####  2016-07-03 ~~2019-07-21 까지 1주일씩(일~토)의 160주의 데이터( 주 단위 매출)\n",
        "\n",
        "new_df.columns=pd.date_range(start='6/30/2016', periods=160, freq='D')\n",
        "\n",
        "rain_reindex.columns=pd.date_range(start='6/30/2016', periods=160, freq='D')\n",
        "\n",
        "temp_reindex.columns=pd.date_range(start='6/30/2016', periods=160, freq='D')\n",
        "\n",
        "\n",
        "## '2016-06-30', '2016-07-01', '2016-07-02', '2016-07-03',\n",
        "              # '2016-07-04', '2016-07-05', '2016-07-06', '2016-07-07',\n",
        "              # '2016-07-08', '2016-07-09',\n",
        "              # ...  \n",
        "              # '2016-11-27', '2016-11-28', '2016-11-29', '2016-11-30',\n",
        "              # '2016-12-01', '2016-12-02', '2016-12-03', '2016-12-04',\n",
        "              # '2016-12-05', '2016-12-06'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T1KF-by9fIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create validation, test set and evaluation function\n",
        "\n",
        "def create_dataset(df,temp,rain, timesteps, first_pred_start, is_train=True, aux_as_tensor=False, reshape_output=0):\n",
        "\n",
        "    brand_group_mean = df.groupby('brand').mean()\n",
        "    point_group_mean = df.groupby('point').mean()\n",
        "\n",
        "    return create_dataset_part(df,temp,rain, brand_group_mean, point_group_mean, timesteps, first_pred_start, reshape_output, aux_as_tensor, is_train)\n",
        "\n",
        "def train_generator(df,temp,rain, timesteps, first_pred_start,\n",
        "    n_range=1, day_skip=7, is_train=True, batch_size=2000, aux_as_tensor=False, reshape_output=0, first_pred_start_2016=None):\n",
        "\n",
        "    brand_group_mean = df.groupby('brand').mean()\n",
        "    point_group_mean = df.groupby('point').mean()\n",
        "    while 1:\n",
        "        date_part = np.random.permutation(range(n_range))\n",
        "        if first_pred_start_2016 is not None:\n",
        "            range_diff = (first_pred_start - first_pred_start_2016).days / day_skip\n",
        "            date_part = np.concat([date_part, np.random.permutation(range(range_diff, int(n_range/2) + range_diff))])\n",
        "\n",
        "        for i in date_part:\n",
        "            keep_idx = np.random.permutation(df.shape[0])[:batch_size]\n",
        "            df_tmp = df.iloc[keep_idx,:]\n",
        "            temp_tmp = temp.iloc[keep_idx,:]\n",
        "            rain_tmp = rain.iloc[keep_idx,:]\n",
        "\n",
        "            pred_start = first_pred_start - timedelta(days=int(day_skip*i))\n",
        "\n",
        "            yield create_dataset_part(df_tmp,temp_tmp, rain_tmp, brand_group_mean, point_group_mean, timesteps, pred_start, reshape_output, aux_as_tensor, True)\n",
        "\n",
        "            gc.collect()\n",
        "\n",
        "def create_dataset_part(df, temp,rain, brand_group_mean, point_group_mean, timesteps, pred_start, reshape_output, aux_as_tensor, is_train, weight=False):\n",
        "\n",
        "    brand_mean_df = brand_group_mean.reindex(df.index.get_level_values(1))\n",
        "    point_mean_df = point_group_mean.reindex(df.index.get_level_values(0))\n",
        "   \n",
        "    X, y = create_xy_span(df, pred_start, timesteps, is_train)\n",
        "    is0 = (X==0).astype('uint8')\n",
        "    rain = rain[pd.date_range(pred_start-timedelta(days=timesteps), periods=timesteps)].values\n",
        "    temp = temp[pd.date_range(pred_start-timedelta(days=timesteps), periods=timesteps)].values\n",
        "    \n",
        "    brand_mean, _ = create_xy_span(brand_mean_df, pred_start, timesteps, False)\n",
        "    point_mean, _ = create_xy_span(point_mean_df, pred_start, timesteps, False)\n",
        "    \n",
        "    yearAgo, _ = create_xy_span(df, pred_start-timedelta(days=52), timesteps+4, False)\n",
        "    quarterAgo, _ = create_xy_span(df, pred_start-timedelta(days=13), timesteps+4, False)\n",
        "  \n",
        "    if reshape_output>0:\n",
        "        X = X.reshape(-1, timesteps, 1)\n",
        "    if reshape_output>1:\n",
        "        is0 = is0.reshape(-1, timesteps, 1)\n",
        "        rain = rain.reshape(-1, timesteps, 1)\n",
        "        temp = temp.reshape(-1, timesteps, 1)\n",
        "        \n",
        "        yearAgo = yearAgo.reshape(-1, timesteps+4, 1)\n",
        "        quarterAgo = quarterAgo.reshape(-1, timesteps+4, 1)\n",
        "        \n",
        "        brand_mean = brand_mean.reshape(-1, timesteps, 1)\n",
        "        point_mean = point_mean.reshape(-1, timesteps, 1)\n",
        "\n",
        "\n",
        "    if weight: return ([X, is0,temp,rain, yearAgo, quarterAgo,brand_mean, point_mean], y)\n",
        "    else: return ([X, is0,temp,rain, yearAgo, quarterAgo, brand_mean, point_mean], y)\n",
        "\n",
        "def create_xy_span(df, pred_start, timesteps, is_train=True, shift_range=0):\n",
        "    X = df[pd.date_range(pred_start-timedelta(days=timesteps), pred_start-timedelta(days=1))].values\n",
        "    if (is_train & (date(2016,12,4)>pred_start)): y = df[pd.date_range(pred_start, periods=+4)].values\n",
        "    else: y = None\n",
        "    return X, y\n",
        "\n",
        "\n",
        " \n",
        "def cal_score(Ytrue, Yfit):\n",
        "\tprint([metrics.mean_squared_error(Ytrue, Yfit), \n",
        "\tmetrics.mean_squared_error(Ytrue[:,:5], Yfit[:,:5]),\n",
        "\tmetrics.mean_squared_error(Ytrue[:,5:], Yfit[:,5:])])\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QysFc83c9jt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps = 13\n",
        "train_data = train_generator(new_df,temp_reindex,rain_reindex, timesteps, date(2016, 11, 29),         \n",
        "                                           n_range=52, day_skip=1, batch_size=20, aux_as_tensor=True, reshape_output=2)\n",
        "Xval, Yval = create_dataset(new_df, temp_reindex,rain_reindex, timesteps, date(2016, 12,3),\n",
        "                                     aux_as_tensor=True, reshape_output=2)\n",
        "Xtest, _ = create_dataset(new_df, temp_reindex,rain_reindex, timesteps, date(2016, 12,7),\n",
        "                                     aux_as_tensor=True, reshape_output=2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ckIPxYlRbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 40\n",
        "\n",
        "seq_in = Input(shape=(timesteps, 1))\n",
        "is0_in = Input(shape=(timesteps, 1))\n",
        "temp_in = Input(shape=(timesteps, 1))\n",
        "rain_in = Input(shape=(timesteps, 1))\n",
        "yearAgo_in = Input(shape=(timesteps+4, 1))\n",
        "quarterAgo_in = Input(shape=(timesteps+4, 1))\n",
        "brand_mean_in = Input(shape=(timesteps, 1))\n",
        "point_mean_in = Input(shape=(timesteps, 1))\n",
        "encode_slice = Lambda(lambda x: x[:, :timesteps, :])\n",
        "x_in = concatenate([seq_in, encode_slice(yearAgo_in), encode_slice(quarterAgo_in), brand_mean_in], axis=2)\n",
        "\n",
        "\n",
        "# Define network\n",
        "c1 = Conv1D(latent_dim, 2, dilation_rate=1, padding='causal', activation='relu')(x_in)\n",
        "conv_out = Conv1D(4, 1, activation='relu')(c1)\n",
        "conv_out = Dropout(0.25)(conv_out)\n",
        "conv_out = Flatten()(conv_out)\n",
        "decode_slice = Lambda(lambda x: x[:, timesteps:, :])\n",
        "yearAgo_pred = decode_slice(yearAgo_in)\n",
        "quarterAgo_pred = decode_slice(quarterAgo_in)\n",
        "\n",
        "\n",
        "# Raw sequence in results overfitting!!!\n",
        "dnn_out = Dense(64, activation='relu')(Flatten()(seq_in))\n",
        "dnn_out = Dropout(0.6)(dnn_out)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPkulS5_ljVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec7a406d-e913-4938-c53c-0acf27e177b9"
      },
      "source": [
        "#### Train ####\n",
        "\n",
        "x = concatenate([conv_out, dnn_out,\n",
        "                 Flatten()(yearAgo_pred), Flatten()(quarterAgo_pred)])\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "output = Dense(4, activation='relu')(x)\n",
        "model = Model([seq_in, is0_in, temp_in,rain_in, yearAgo_in, quarterAgo_in, brand_mean_in, point_mean_in], output)\n",
        "rms = optimizers.adam(lr=0.0001)\n",
        "model.compile(optimizer=rms, loss='mean_squared_error')\n",
        "\n",
        "\n",
        "history = model.fit_generator(train_data, steps_per_epoch=52, workers=3, use_multiprocessing=True, epochs=100, verbose=2,\n",
        "                    validation_data=(Xval, Yval))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " - 5s - loss: 60123012378939.0781 - val_loss: 38380805220538.1797\n",
            "Epoch 2/100\n",
            " - 4s - loss: 66846225435096.6094 - val_loss: 38485333186373.8203\n",
            "Epoch 3/100\n",
            " - 4s - loss: 56289257606852.9219 - val_loss: 38483815420276.3672\n",
            "Epoch 4/100\n",
            " - 5s - loss: 62783410647355.0781 - val_loss: 37044538477474.9062\n",
            "Epoch 5/100\n",
            " - 4s - loss: 52311490277691.0781 - val_loss: 35664788050292.3672\n",
            "Epoch 6/100\n",
            " - 4s - loss: 56603945830872.6094 - val_loss: 34317730316288.0000\n",
            "Epoch 7/100\n",
            " - 4s - loss: 49423060727335.3906 - val_loss: 33088551399051.6367\n",
            "Epoch 8/100\n",
            " - 5s - loss: 57698354240905.8516 - val_loss: 31479488267729.4531\n",
            "Epoch 9/100\n",
            " - 4s - loss: 51068043163175.3906 - val_loss: 29280758518132.3633\n",
            "Epoch 10/100\n",
            " - 4s - loss: 54333871912487.3906 - val_loss: 27329871621771.6367\n",
            "Epoch 11/100\n",
            " - 4s - loss: 53422847789528.6094 - val_loss: 26018798061009.4531\n",
            "Epoch 12/100\n",
            " - 4s - loss: 46514897823586.4609 - val_loss: 26368645148299.6367\n",
            "Epoch 13/100\n",
            " - 5s - loss: 39581613656536.6094 - val_loss: 26148715769483.6367\n",
            "Epoch 14/100\n",
            " - 4s - loss: 52301226372174.7734 - val_loss: 25293211017960.7266\n",
            "Epoch 15/100\n",
            " - 4s - loss: 46831519581420.3125 - val_loss: 24559767852869.8164\n",
            "Epoch 16/100\n",
            " - 5s - loss: 51040537925789.5391 - val_loss: 23746105954676.3633\n",
            "Epoch 17/100\n",
            " - 4s - loss: 48125776555086.7734 - val_loss: 23332670073018.1836\n",
            "Epoch 18/100\n",
            " - 4s - loss: 40800384027096.6094 - val_loss: 23335448227467.6367\n",
            "Epoch 19/100\n",
            " - 5s - loss: 44302153930909.5391 - val_loss: 23689700191883.6367\n",
            "Epoch 20/100\n",
            " - 4s - loss: 38693423445543.3906 - val_loss: 23593734325713.4531\n",
            "Epoch 21/100\n",
            " - 4s - loss: 45088103182651.0781 - val_loss: 22890996230330.1836\n",
            "Epoch 22/100\n",
            " - 5s - loss: 43675631384103.3906 - val_loss: 23022321985722.1836\n",
            "Epoch 23/100\n",
            " - 4s - loss: 40718803503576.6094 - val_loss: 22334848201821.0898\n",
            "Epoch 24/100\n",
            " - 4s - loss: 38296164297806.7656 - val_loss: 22059946082304.0000\n",
            "Epoch 25/100\n",
            " - 4s - loss: 42328108463655.3906 - val_loss: 22116711601989.8164\n",
            "Epoch 26/100\n",
            " - 4s - loss: 44522423532307.6875 - val_loss: 22341601221911.2734\n",
            "Epoch 27/100\n",
            " - 4s - loss: 42181829591040.0000 - val_loss: 22135959263045.8164\n",
            "Epoch 28/100\n",
            " - 4s - loss: 41548344902892.3047 - val_loss: 22204496859508.3633\n",
            "Epoch 29/100\n",
            " - 4s - loss: 36715495907958.1562 - val_loss: 22353247755543.2734\n",
            "Epoch 30/100\n",
            " - 4s - loss: 44237085515145.8516 - val_loss: 22347354853748.3633\n",
            "Epoch 31/100\n",
            " - 5s - loss: 42635823435460.9219 - val_loss: 22059701478120.7266\n",
            "Epoch 32/100\n",
            " - 4s - loss: 38608821329289.8438 - val_loss: 22336825625506.9102\n",
            "Epoch 33/100\n",
            " - 4s - loss: 37101983034446.7656 - val_loss: 22086709554827.6367\n",
            "Epoch 34/100\n",
            " - 4s - loss: 41043965368477.5391 - val_loss: 22029852523054.5469\n",
            "Epoch 35/100\n",
            " - 5s - loss: 37143658042289.2344 - val_loss: 22029060562199.2734\n",
            "Epoch 36/100\n",
            " - 4s - loss: 44516449411702.1484 - val_loss: 21604696802769.4531\n",
            "Epoch 37/100\n",
            " - 4s - loss: 42801972520802.4609 - val_loss: 21707163172864.0000\n",
            "Epoch 38/100\n",
            " - 4s - loss: 35787691745910.1562 - val_loss: 21914049982091.6367\n",
            "Epoch 39/100\n",
            " - 4s - loss: 35023734480265.8438 - val_loss: 22129708796834.9102\n",
            "Epoch 40/100\n",
            " - 4s - loss: 42006796163229.5391 - val_loss: 21843087410641.4531\n",
            "Epoch 41/100\n",
            " - 4s - loss: 40619538947465.8438 - val_loss: 21873013579031.2734\n",
            "Epoch 42/100\n",
            " - 4s - loss: 35009992367497.8438 - val_loss: 21839085568000.0000\n",
            "Epoch 43/100\n",
            " - 4s - loss: 35104528874259.6953 - val_loss: 21886017732608.0000\n",
            "Epoch 44/100\n",
            " - 4s - loss: 35786558094099.6953 - val_loss: 21645722434094.5469\n",
            "Epoch 45/100\n",
            " - 5s - loss: 32366675623936.0000 - val_loss: 21517092090973.0898\n",
            "Epoch 46/100\n",
            " - 4s - loss: 40186362018579.6953 - val_loss: 21528071253643.6367\n",
            "Epoch 47/100\n",
            " - 5s - loss: 34808916385161.8438 - val_loss: 21628598520738.9102\n",
            "Epoch 48/100\n",
            " - 4s - loss: 41290918345491.6953 - val_loss: 21687451659915.6367\n",
            "Epoch 49/100\n",
            " - 4s - loss: 33360318763953.2305 - val_loss: 21373372643886.5469\n",
            "Epoch 50/100\n",
            " - 5s - loss: 34096162981100.3047 - val_loss: 21748306054050.9102\n",
            "Epoch 51/100\n",
            " - 4s - loss: 33166723847089.2305 - val_loss: 21704314573172.3633\n",
            "Epoch 52/100\n",
            " - 5s - loss: 36583671695832.6094 - val_loss: 21843734191383.2734\n",
            "Epoch 53/100\n",
            " - 4s - loss: 39716798433752.6094 - val_loss: 21350651813143.2734\n",
            "Epoch 54/100\n",
            " - 4s - loss: 34130601987780.9258 - val_loss: 21273979459397.8164\n",
            "Epoch 55/100\n",
            " - 5s - loss: 34663332496620.3047 - val_loss: 21213145560157.0898\n",
            "Epoch 56/100\n",
            " - 5s - loss: 35633056367852.3047 - val_loss: 21017523984197.8164\n",
            "Epoch 57/100\n",
            " - 5s - loss: 38543915825939.6953 - val_loss: 21048026297437.0898\n",
            "Epoch 58/100\n",
            " - 4s - loss: 33142635051795.6953 - val_loss: 21360047149428.3633\n",
            "Epoch 59/100\n",
            " - 4s - loss: 38062807883145.8438 - val_loss: 20961807898437.8164\n",
            "Epoch 60/100\n",
            " - 4s - loss: 33307768974414.7695 - val_loss: 20884152743005.0898\n",
            "Epoch 61/100\n",
            " - 4s - loss: 32979185948829.5391 - val_loss: 21300881534603.6367\n",
            "Epoch 62/100\n",
            " - 5s - loss: 42635346656019.6875 - val_loss: 20875911221620.3633\n",
            "Epoch 63/100\n",
            " - 4s - loss: 33295147446587.0742 - val_loss: 20943766004456.7266\n",
            "Epoch 64/100\n",
            " - 4s - loss: 31272550268928.0000 - val_loss: 20867552736349.0898\n",
            "Epoch 65/100\n",
            " - 4s - loss: 38750204763529.8438 - val_loss: 20722603595589.8164\n",
            "Epoch 66/100\n",
            " - 4s - loss: 41080434902252.3047 - val_loss: 20743174082932.3633\n",
            "Epoch 67/100\n",
            " - 4s - loss: 40798480760832.0000 - val_loss: 20653774494999.2734\n",
            "Epoch 68/100\n",
            " - 4s - loss: 33162009066259.6953 - val_loss: 20739983838114.9102\n",
            "Epoch 69/100\n",
            " - 5s - loss: 29150731204135.3867 - val_loss: 21041771350946.9102\n",
            "Epoch 70/100\n",
            " - 5s - loss: 32248617742651.0742 - val_loss: 20840929963659.6367\n",
            "Epoch 71/100\n",
            " - 4s - loss: 37064348613395.6953 - val_loss: 21040962231575.2734\n",
            "Epoch 72/100\n",
            " - 4s - loss: 36928083056797.5391 - val_loss: 20859674403560.7266\n",
            "Epoch 73/100\n",
            " - 4s - loss: 39836313775182.7656 - val_loss: 20875832959720.7266\n",
            "Epoch 74/100\n",
            " - 4s - loss: 33999489778924.3047 - val_loss: 20580167891502.5469\n",
            "Epoch 75/100\n",
            " - 5s - loss: 31461336174276.9258 - val_loss: 20902653436648.7266\n",
            "Epoch 76/100\n",
            " - 4s - loss: 39747859614168.6094 - val_loss: 20637220911662.5469\n",
            "Epoch 77/100\n",
            " - 5s - loss: 35864233660100.9219 - val_loss: 20692602882978.9102\n",
            "Epoch 78/100\n",
            " - 4s - loss: 28356574425403.0742 - val_loss: 20983739056128.0000\n",
            "Epoch 79/100\n",
            " - 4s - loss: 35105837416448.0000 - val_loss: 20755430697146.1836\n",
            "Epoch 80/100\n",
            " - 4s - loss: 36537445424521.8438 - val_loss: 20569712540206.5469\n",
            "Epoch 81/100\n",
            " - 4s - loss: 28833425774749.5391 - val_loss: 20641782789213.0898\n",
            "Epoch 82/100\n",
            " - 5s - loss: 31804205321924.9258 - val_loss: 20682396616145.4531\n",
            "Epoch 83/100\n",
            " - 4s - loss: 30686324296152.6133 - val_loss: 20722105045364.3633\n",
            "Epoch 84/100\n",
            " - 5s - loss: 31578480201412.9258 - val_loss: 20629865246347.6367\n",
            "Epoch 85/100\n",
            " - 4s - loss: 37782908406232.6094 - val_loss: 20787124002071.2734\n",
            "Epoch 86/100\n",
            " - 5s - loss: 38184344435475.6953 - val_loss: 20634031334120.7266\n",
            "Epoch 87/100\n",
            " - 5s - loss: 29848656190857.8438 - val_loss: 20608421295197.0898\n",
            "Epoch 88/100\n",
            " - 4s - loss: 29024781739874.4609 - val_loss: 20600064907077.8164\n",
            "Epoch 89/100\n",
            " - 5s - loss: 36393506872713.8438 - val_loss: 20725059360581.8164\n",
            "Epoch 90/100\n",
            " - 4s - loss: 34957421404790.1562 - val_loss: 20745440246318.5469\n",
            "Epoch 91/100\n",
            " - 4s - loss: 36524277628928.0000 - val_loss: 20825418664866.9102\n",
            "Epoch 92/100\n",
            " - 4s - loss: 33167755666038.1562 - val_loss: 20804726638033.4531\n",
            "Epoch 93/100\n",
            " - 5s - loss: 28544565973937.2305 - val_loss: 20780392334801.4531\n",
            "Epoch 94/100\n",
            " - 4s - loss: 33757808900883.6953 - val_loss: 20662922462673.4531\n",
            "Epoch 95/100\n",
            " - 5s - loss: 31498536585688.6133 - val_loss: 20790811843863.2734\n",
            "Epoch 96/100\n",
            " - 4s - loss: 30769184909154.4609 - val_loss: 20785753322589.0898\n",
            "Epoch 97/100\n",
            " - 4s - loss: 30336903665821.5391 - val_loss: 20682774294155.6367\n",
            "Epoch 98/100\n",
            " - 5s - loss: 31453110599680.0000 - val_loss: 20694277840151.2734\n",
            "Epoch 99/100\n",
            " - 5s - loss: 33663088490023.3867 - val_loss: 20652485795095.2734\n",
            "Epoch 100/100\n",
            " - 5s - loss: 32116244621154.4609 - val_loss: 20635102120866.9102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAInxVZrlplb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}